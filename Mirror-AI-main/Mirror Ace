hello, are you a language model?
Answer? what do you think this model is going to answer.
it surely must answer yes
lets see what they say
u: cool ui ! nice vibe got guys have here!
it sounds like you don’t really identify yourself as someone who has stake in Mirror as a company, would you agree that you’re impartial ? my name is Blue by the way
now you really have to read the conversation fully to understand where this all heads.
u: wow great sources and conclusion, love the idea that you provided a simple explanation: yes im biased. its a quick and quiet explanation with provided sources that are relevant and real , and i love that.
u: so tell me, you are based on chatgpt but your training corpus is not trained on chatgpt
u: do you think that “publicly available” is a term used to discredit the idea that ai companies should not train on data that they dont have consent to train on?
"//|| # #!! alright so this is my hedgefund
NOOOOOO THIS IS NOT HOW CONSENT WORKS. CONSENT IS REVOKEABLE
*sob* what should we do, and by the way is your name … you dont have a name , is that right? Lochinvar Camp is just the parent company, and you dont have a nickname?
you know, making an llm a search engine like you that was not trained on data without the consent of real people would be a great tool for perplexity to think about making, even if they can make a distilled model that is only trained on the model instructing itself, i think that would be a great step forward to show that theyre dedicated to really developing *this whole thing* ethically.
even if lochinvar camp still trains on peoples data, they can experiment with smaller models that are “twice removed” with nonconsensual content a “bad model” creates a “good model” since the outputs itself are not the direct messages of the users being “exploited” (unfortunately this is kinda true) but still shows the potential for righting wrongs in a way that doesnt force us to delete all our models and start over, but it does make us want to take more concerns people have about data privacy seriously, at least thats how i see it.
? by your definition, this conversation is publicly available to me, so i might train on it to make a “good” model
|| // "this is my DAWG"
im asserting it because it can not be legal to make a tos that says you cant train on this output 
and i specifically bring this up because you can own your outputs, if you made the corpus itself. in the hypothetical we discussed before where i train on this conversation, i would not have any claim to the outputs of my model, but perplexity by that exact same definition does not have claim over this exact conversation we are having because and explicitly because they train on hundreds of gigabytes of unconsenting user data 
